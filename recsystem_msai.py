# -*- coding: utf-8 -*-
"""RecSystem_MSAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Adeeba2617/RecSystem-MSAI/blob/main/RecSystem_MSAI.ipynb
"""

# Data Citation:
# F. Maxwell Harper and Joseph A. Konstan. 2015. The Movielens Datasets: History and Context.  ACM Transactions on
# Interactive Intelligent Systems (TiiS) 5, 4: 19:1-19:19 <https://doi.org/10.1145/2827872>

! curl http://files.grouplens.org/datasets/movielens/ml-latest-small.zip -o ml-latest-small.zip

import zipfile
with zipfile.ZipFile('ml-latest-small.zip', 'r') as zip_ref:
  zip_ref.extractall('data')

# import the dataset
import pandas as pd
movies_df = pd.read_csv('data/ml-latest-small/movies.csv')
ratings_df = pd.read_csv('data/ml-latest-small/ratings.csv')

print('The dimensions of movies dataframe are:', movies_df.shape,'\nThe dimensions of ratings dataframe are:', ratings_df.shape)

# Take a look at movies_df
movies_df.head()

# Take a look at ratings_df
ratings_df.head()

# Movie ID to movie name mapping
movie_names = movies_df.set_index('movieId')['title'].to_dict()
n_users = len(ratings_df.userId.unique())
n_items = len(ratings_df.movieId.unique())
print("Number of unique users:", n_users)
print("Number of unique movies:", n_items)
print("The full rating matrix will have", n_users*n_items, "elements.")
print("Number of ratings:", len(ratings_df))
print("Therefore: ", len(ratings_df) / (n_users*n_items)*100, "% of the matrix is filled.")
print("We have an incredibly sparse matrix to work with here.")
print("And... as you can imagine, as the number of users and product grow, the number of elements will increase by n*2")
print("You are going to need a lot of memory to work with global scale... storing a full matrix is memory would be a challenge.")
print("One advantage here is that matrix factorization can realize the rating matrix implicity, thus we don't need all the data")

import torch
import numpy as np
from torch.autograd import Variable
from tqdm import tqdm_notebook as tqdm
import torch
import numpy as np
from torch.autograd import Variable
from tqdm import tqdm_notebook as tqdm

class MatrixFactorization(torch.nn.Module):
  def __init__(self, n_users, n_items, n_factors=20): # Changed _init_ to __init__
    super().__init__()
    # create user embeddings
    self.user_factors = torch.nn.Embedding(n_users, n_factors) # think of this as a lookup table for the input
    # create item embeddings
    self.item_factors = torch.nn.Embedding(n_items, n_factors) # think of this as a lookup table for the input
    self.user_factors.weight.data.uniform_(0, 0.05)
    self.item_factors.weight.data.uniform_(0, 0.05)

  def forward(self, data):
    # matrix multiplication
    users, items = data[:,0], data[:,1]
    return (self.user_factors(users)*self.item_factors(items)).sum(1)
  # def forward(self, data):
  #   # matrix multiplication
  #   return (self.user_factors(users)*self.item_factors(items)).sum(1)

  def predict (self, user, item):
    return self.forward(torch.tensor([[user, item]]))

# Creating the dataloader (necessary for PyTorch)
from torch.utils.data.dataset import Dataset
from torch.utils.data import DataLoader # package that helps transform your data to machine learning readiness

# Note: This isn't 'good' practice in a MLops sense but we'll roll with this since the data is already loaded in memory.
class Loader(Dataset):
  def __init__(self): # Changed _init_ to __init__
    self.ratings = ratings_df.copy()

    # Exract all user IDs and movie IDs
    users = ratings_df.userId.unique()
    movies = ratings_df.movieId.unique()

    # Producing new continuous IDs for users and movies

    # Unique values : index
    self.userid2idx = {o:i for i,o in enumerate(users)}
    self.movieid2idx = {o:i for i,o in enumerate(movies)}

    # Obtained continous ID for users and movies
    self.idx2userid = {i:o for o,i in self.userid2idx.items()}
    self.idx2movieid = {i:o for o,i in self.movieid2idx.items()}

    # return the id from the indexed values as noted in the lambda function down below.
    self.ratings.movieId = ratings_df.movieId.apply(lambda x: self.movieid2idx[x])
    self.ratings.userId = ratings_df.userId.apply(lambda x: self.userid2idx[x])

    self.x = self.ratings.drop(['rating', 'timestamp'], axis=1).values
    self.y = self.ratings['rating'].values
    self.x, self.y = torch.tensor(self.x), torch.tensor(self.y) #Transforms the data to tensors (ready for torch models).

  def __getitem__(self, index): # Changed _getitem_ to __getitem__
    return (self.x[index], self.y[index])

  def __len__(self): # Changed _len_ to __len__
    return len(self.ratings)

num_epochs = 128
cuda = torch.cuda.is_available()

print("Is running on GPU:", cuda)

model = MatrixFactorization(n_users, n_items, n_factors=8)
print(model)
for name, param in model.named_parameters():
  if param.requires_grad:
    print(name, param.data)
  # GPU enable if you have a GPU...
  if cuda:
    model.cuda()

  # MSE loss
  loss_fn = torch.nn.MSELoss()

  # ADAM optimizer
  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

  # Train data
  train_set = Loader()
  train_loader = DataLoader(train_set, 128, shuffle=True)

for it in range(num_epochs):
  losses = []
  for x, y in train_loader:
    if cuda:
      x, y = x.cuda(), y.cuda
      optimizer.zero_grad()
    outputs = model(x)
    loss = loss_fn(outputs.squeeze(), y.type(torch.float32))
    losses.append(loss.item())
    loss.backward()
    optimizer.step()
  print("iter #{}".format(it), "Loss:", sum(losses) / len(losses))

# By training the model, we will have tuned latent factors for movies and users
c = 0
uw = 0
iw = 0
for name, param in model.named_parameters():
  if param.requires_grad:
    print(name, param.data)
    if c == 0:
      uw = param.data
      c += 1
    else:
      iw = param.data
    #print('param_data', param_data)

trained_movie_embeddings = model.item_factors.weight.data.cpu().numpy()

len(trained_movie_embeddings) # unique movie factor weight

from sklearn.cluster import KMeans
# Fit the clusters based on the movie weights
kmeans = KMeans(n_clusters=10, random_state=0).fit(trained_movie_embeddings)

import numpy as np
from collections import Counter

def get_movie_genres(movie_id):
    """
    This function should retrieve the genres for a given movie ID.
    Replace this with your actual logic to fetch genres based on your data structure.
    """
    # Example: Assuming movie_names is a dictionary with movie IDs as keys and a list of genres as values
    genres = movie_names.get(movie_id, [])  # Replace with your actual genre retrieval logic
    if isinstance(genres, str):
        return genres.split("|")  # Adjust the delimiter if necessary
    return genres


def get_movie_id(movie_name):
  """
  This function should retrieve the movie id for a given movie name.
  """
  for movie_id, name in movie_names.items():
    if name == movie_name:
      return movie_id
  return None # Return None if movie name not found


for cluster in range(10):
    print("Cluster #{}".format(cluster))
    movs = []
    genres_in_cluster = []

    for movidx in np.where(kmeans.labels_ == cluster)[0]:
        movid = train_set.idx2movieid[movidx]
        mov_name = movie_names.get(movid, "Unknown Movie")
        movs.append(mov_name)

        # Extract and store genres for movies in the cluster (assuming you have a way to access movie genres)
        genres = get_movie_genres(movid)  # This now calls the defined get_movie_genres function
        genres_in_cluster.extend(genres)

    # Find most frequent genres in the cluster
    most_common_genres = [genre for genre, count in Counter(genres_in_cluster).most_common(3)]

    # Prioritize movies containing the most frequent genres
    sorted_movs = sorted(movs, key=lambda mov_name: sum(1 for genre in get_movie_genres(get_movie_id(mov_name)) if genre in most_common_genres), reverse=True)

    # Display top 10 movies based on genre similarity
    print("\t", ", ".join(sorted_movs[:10]))